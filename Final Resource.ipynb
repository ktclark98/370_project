{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What makes a Kickstarter campaign successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of the project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on how the data was cleaned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Code/link to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of the data exploration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Visualization #1 code here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the visualization here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Modeling code here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the statistical model and what it means..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do any actual machine learning, we have to first split our data into a test set, and a training set. Since our data set is particularly large, I chose to make the training set a little bit bigger because that will allow the model to have a better fit without worrying about possibility of over fitting. There's still a lot of data in the test set to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data[['category', 'main_category', 'currency', 'goal','duration', 'country', 'month_launched', 'month_deadline', 'launch_day_of_week', 'count_x']]\n",
    "outcome = data[['state']]\n",
    "feature_name = data[[\"name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can this pretty easily by first selecting all the relevant features and put those into a data frame. Next, we can put the outcome of the data into it's own data frame as well. We can then split the data into a training set and a test set using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_small, test_features, train_outcome_small, test_outcome = train_test_split(\n",
    "    feature,\n",
    "    outcome,\n",
    "    test_size = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried many different many different machine learning algorithms over the course of this study but none performed as well as Random Forest. The Random Forest is fairly similar to a Decision Tree algorithm but differs in that a Random Forest builds multiple decision trees in order to get a more accurate prediction.\n",
    "\n",
    "When tuning the algorithm, we tried various parameters to see which would get us the most accurate model. After a while we found that the best parameters for our model were...\n",
    "\n",
    "Variance Threshold = .1\n",
    "SelectKBest = 10\n",
    "Random Forest Classifier N-estimators = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soda\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-71894e2bd7f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m pipe = make_pipeline(VarianceThreshold(.1),\n\u001b[0m\u001b[0;32m      4\u001b[0m                      \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[0mrfc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_features=\"auto\",max_depth=4, min_samples_split=2)\n",
    "pipe = make_pipeline(VarianceThreshold(.1),\n",
    "                     SelectKBest(k=10),\n",
    "                     rfc\n",
    "                     )\n",
    "\n",
    "params ={'randomforestclassifier__n_estimators':[40],\n",
    "        }\n",
    "\n",
    "grid_search_rf = GridSearchCV(pipe, params,\n",
    "                               cv = KFold(n_splits=10, shuffle=True),n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting the GridSearch, we then fit our model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.fit(train_features_small,train_outcome_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then used this to predict based on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.score(validation_features,validation_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an accuracy score of 66%, we can see that our model does a pretty good job at predicting the success or failure of a kickstarter event. But it might be more interesting to see where the model does where it doesn't. \n",
    "\n",
    "First we can start by adding a column of our predictions and readding the actual outcome to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features['preds'] = grid_search_rf.predict(test_features)\n",
    "test_features['actual'] = test_outcome['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate measures such as Sensitivity and Specificity. Sensitivity, also referred to as the true positive rate, tells us, of all of the cases in the data, how many did we accurately predict? This indicates the model's ability to detect cases. In other words, how sensitively does the model pick up on cases?\n",
    "\n",
    "Specificity, also referred to as the true negative rate, tells us, of all of the non-cases in the data, how many did we accurately predict? This indicates the model's ability to assign non-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = test_features.loc[test_features['actual'] == test_features['preds']][['actual', 'preds']]\n",
    "false_df = test_features.loc[test_features['actual'] != test_features['preds']][['actual', 'preds']]\n",
    "\n",
    "true_positives = len(true_df[true_df['preds']==1])\n",
    "false_negatives = len(false_df[false_df['preds']==0])\n",
    "true_negatives = len(true_df[true_df['preds']==0])\n",
    "false_positives = len(false_df[false_df['preds']==1])\n",
    "\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "specificity = true_negatives / (true_negatives + false_positives)\n",
    "\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "print(f'Specificity: {specificity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating these metrics we get a sensitivity value of .136 and a specificity value of .957. This gives is a much clearer understanding of where our model is effective. These values illustrate that our model does not accurately predict positive cases but is really accurate at predicting when a project will fail. This makes a lot of sense because most projects that get created by kickstarter tend to fail so it makes sense that it would be hard to predict whethere a project will succeed just based on the features that we have available to us.\n",
    "\n",
    "We can visualize this by looking at the ROC Curve. The ROC Curve is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true=test_features.actual, y_score=test_features.preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Draw your ROC curve\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model does predict more true positives than false positives, but the rate is close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *About the Team*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the Team..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
